{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 Final Project\n",
    "## Using Remote Sensing to Predict and Understand Future Wildfires\n",
    "#### by Zeynep Ankut, Isaiah Stene, Corey Becker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import & Setup Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importFunctions as imp \n",
    "import kagglehub\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"fantineh/next-day-wildfire-spread\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "train_files = path + \"/next_day_wildfire_spread_train*\"\n",
    "test_files = path + \"/next_day_wildfire_spread_test*\"\n",
    "eval_files = path + \"/next_day_wildfire_spread_eval*\"\n",
    "\n",
    "\n",
    "train = imp.get_dataset(train_files,  \n",
    "      data_size=64,\n",
    "      sample_size=32,\n",
    "      batch_size=100,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False)\n",
    "\n",
    "test = imp.get_dataset(test_files,  \n",
    "      data_size=64,\n",
    "      sample_size=32,\n",
    "      batch_size=100,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False)\n",
    "\n",
    "\n",
    "eval_data = imp.get_dataset(test_files,  \n",
    "      data_size=64,\n",
    "      sample_size=32,\n",
    "      batch_size=100,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot rows of the train data\n",
    "def plotRows(start,end,titles,n_features,data):\n",
    "  inputs, labels = next(iter(data))\n",
    "\n",
    "  fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "  CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "  BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "  NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "\n",
    "  n_rows = end-start\n",
    "  for i in range(n_rows):\n",
    "    for j in range(n_features + 1):\n",
    "      plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "      if i == 0:\n",
    "        plt.title(titles[j], fontsize=13)\n",
    "      if j < n_features - 1:\n",
    "        plt.imshow(inputs[i+start, :, :, j], cmap='viridis')\n",
    "      if j == n_features - 1:\n",
    "        plt.imshow(inputs[i+start, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "      if j == n_features:\n",
    "        plt.imshow(labels[i+start, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "      plt.axis('off')\n",
    "  plt.tight_layout()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]\n",
    "\n",
    "plotRows(20,25,titles,len(titles)-1, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRESHHOLD\n",
    "THRESHOLD = .4 #what sigmoid needs to return to count it as fire\n",
    "\n",
    "#gets the train data \n",
    "inputs, labels = next(iter(train))\n",
    "indices = [0, 2, 4, 6, 7, 9, 10, 11]\n",
    "tf.gather(inputs, indices, axis=-1)\n",
    "\n",
    "#THIS IS ADDING MORE FIRE DATA TO TRAIN ON\n",
    "\n",
    "min_fire_pixels = 15\n",
    "\n",
    "fire_mask = labels[..., 0]  # Shape: (N, 32, 32)\n",
    "fire_tiles = np.sum(fire_mask, axis=(1, 2)) > min_fire_pixels  # Shape: (N,)\n",
    "\n",
    "copies = 2\n",
    "\n",
    "#only gets fire data\n",
    "inputs_fire = inputs[fire_tiles]\n",
    "labels_fire = labels[fire_tiles]\n",
    "\n",
    "# Duplicate them\n",
    "fire_inputs_aug = np.tile(inputs_fire, (copies, 1, 1, 1))\n",
    "fire_labels_aug = np.tile(labels_fire, (copies, 1, 1, 1))\n",
    "\n",
    "# Combine with original data\n",
    "inputs = np.concatenate([inputs, fire_inputs_aug], axis=0)\n",
    "labels = np.concatenate([labels, fire_labels_aug], axis=0)\n",
    "\n",
    "#gets eval data\n",
    "e_inputs, e_labels = next(iter(eval_data))\n",
    "\n",
    "#also gets eval data with fire only\n",
    "min_fire_pixels_eval = 10\n",
    "\n",
    "# Extract fire mask from eval labels\n",
    "eval_fire_mask = e_labels[..., 0] \n",
    "\n",
    "# Boolean mask: which eval tiles have enough fire\n",
    "eval_fire_tiles = np.sum(eval_fire_mask, axis=(1, 2)) > min_fire_pixels_eval\n",
    "\n",
    "# Filter eval data\n",
    "e_inputs = e_inputs[eval_fire_tiles]\n",
    "e_labels = e_labels[eval_fire_tiles]\n",
    "\n",
    "#get test data\n",
    "t_inputs, t_labels = next(iter(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to build the CNN Models\n",
    "\n",
    "We use the UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes the model, keeps it simple\n",
    "\n",
    "def build_simple_cnn(height,width, features):\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(height, width, features)),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "        layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_larger_cnn(height, width, features):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(height, width, features)),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.SpatialDropout2D(0.2),\n",
    "\n",
    "        layers.Conv2D(64, (3,3), dilation_rate=2, activation='relu', padding='same'),\n",
    "\n",
    "        layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "\n",
    "        layers.Conv2D(1, (1, 1), activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_unet(height, width, channels):\n",
    "    inputs = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "    reg = tf.keras.regularizers.l2(1e-3) #this is a small ridge regularizer\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(inputs)\n",
    "    c1 = layers.BatchNormalization()(c1)\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(c1)\n",
    "    # c1 = layers.Dropout(0.2)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(p1)\n",
    "    c2 = layers.BatchNormalization()(c2)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(c2)\n",
    "    c2 = layers.Dropout(0.2)(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(p2)\n",
    "    b = layers.Dropout(0.2)(b)\n",
    "    b = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(b)\n",
    "\n",
    "    # Decoder\n",
    "    u2 = layers.UpSampling2D((2, 2))(b)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(u2)\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(c3)\n",
    "    c3 = layers.Dropout(0.2)(c3)\n",
    "\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    c4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(u1)\n",
    "    c4 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=reg)(c4)\n",
    "    c5 = layers.conv2DTranpose(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "    c5 = layers.BatchNormalization()(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions, we use a combination of all 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is the loss functions that I will use\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true,y_pred,weight_nofire=1.0,weight_fire = 10.0):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        y_true (Tensor): Ground-truth values\n",
    "        y_pred (Tensor): Predited values\n",
    "        weight_nofire (float): Weight of class 0 (no-fire)\n",
    "        weight_fire (float): Weight of class 1 (fire)\n",
    "        \n",
    "    \"\"\"\n",
    "    bin_crossentropy = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    weights = y_true * weight_fire + (1.0 - y_true) * weight_nofire\n",
    "    weight_bce = weights * bin_crossentropy\n",
    "\n",
    "    return K.mean(weight_bce,axis = 1)\n",
    "\n",
    "\n",
    "def dice_loss(y_true,y_pred):\n",
    "    BATCH_SIZE = 32\n",
    "    smooth = 1e-6\n",
    "\n",
    "    y_true_f = K.reshape(y_true, (BATCH_SIZE, -1))\n",
    "    y_pred_f = K.reshape(y_pred, (BATCH_SIZE, -1))\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f, axis=1)\n",
    "    return 1 - (2. * intersection + smooth) / (K.sum(y_true_f, axis=1) + K.sum(y_pred_f, axis=1) + smooth)\n",
    "    \n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.75, gamma=2.0):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "    return tf.reduce_mean(-alpha * tf.pow(1. - pt, gamma) * tf.math.log(pt))\n",
    "\n",
    "def combined_loss(y_true,y_pred):\n",
    "    #this is a combined loss function, so these weights can be messed with to proritize one loss function over the others\n",
    "    d_weight = 1.25\n",
    "    bce_weight = .75\n",
    "    focal_weight = 1.25\n",
    "    return d_weight * dice_loss(y_true,y_pred) + bce_weight * weighted_bce_loss(y_true,y_pred) + focal_weight * focal_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this removes all the no data points, gets it to be all 1's or zeros\n",
    "\n",
    "def shift_data(labels):\n",
    "    return tf.maximum(labels,tf.constant([0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF METRIC VERSION\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def iou_m(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Calculation of intersection over union metric.\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensor): true fire mask\n",
    "        y_pred  (Tensor): predicted mask\n",
    "    Returns:\n",
    "        (float): IoU metric \n",
    "    \"\"\"\n",
    "    y_pred = tf.cast(y_pred > THRESHOLD, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    iou = tf.math.divide_no_nan(intersection, union)\n",
    "    return iou\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred > THRESHOLD, tf.float32)\n",
    "    y_true = tf.cast(tf.where(y_true < 0, 0.0, y_true), tf.float32)\n",
    "\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    actual_positives = tf.reduce_sum(y_true)\n",
    "\n",
    "    recall = tf.math.divide_no_nan(true_positives, actual_positives)\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred > THRESHOLD, tf.float32)\n",
    "    y_true = tf.cast(tf.where(y_true < 0, 0.0, y_true), tf.float32)\n",
    "\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred, 1)), tf.float32))\n",
    "    predicted_positives = tf.reduce_sum(y_pred)\n",
    "\n",
    "    precision = tf.math.divide_no_nan(true_positives, predicted_positives)\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    prec = precision_m(y_true, y_pred)\n",
    "    rec = recall_m(y_true, y_pred)\n",
    "    f1 = tf.math.divide_no_nan(2 * prec * rec, prec + rec)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Fit the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model \n",
    "n_fires, height,width, feat_images = inputs.shape\n",
    "\n",
    "# model = build_segmentation_cnn(height,width,feat_images,2)\n",
    "model = build_larger_cnn(height,width,feat_images)\n",
    "\n",
    "labels = shift_data(labels)\n",
    "\n",
    "labels_fire = shift_data(labels_fire)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), \n",
    "              loss = combined_loss, \n",
    "              metrics = ['accuracy', precision_m, recall_m, f1_m,iou_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the fit\n",
    "\n",
    "n_epoch = 50 #too many epochs overfit very badly\n",
    "\n",
    "#stops epochs early if the test loss gets worse\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            patience=30,#num epochs of worse val lose before stopping\n",
    "                                            min_delta=0.001, # minimum change to qualify as improvement\n",
    "                                            restore_best_weights=True,\n",
    "                                            verbose = True)\n",
    "\n",
    "#this reduces the learning rate once the data plataues\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    inputs,\n",
    "    labels,\n",
    "    validation_data = (e_inputs,e_labels),\n",
    "    epochs = n_epoch,\n",
    "    batch_size = 16,\n",
    "    verbose = True,\n",
    "    callbacks=[\n",
    "        early_stop,\n",
    "        reduce_lr\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "best_epoch = early_stop.stopped_epoch - early_stop.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training\n",
    "def plot_training_history(history,best_epoch):\n",
    "    \"\"\"\n",
    "    Plots loss and all other metrics in model history.\n",
    "    Accepts a `history` object returned by model.fit().\n",
    "    \"\"\"\n",
    "    history_dict = history.history\n",
    "    metrics = [m for m in history_dict.keys() if not m.startswith('val_') and m != 'loss']\n",
    "\n",
    "    # best_epoch = history.callbacks\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "    plt.axvline(x=best_epoch, ls = '--', label = \"Best Epoch\", color = \"#CBC318\" )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Train Metrics\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for metric in metrics:\n",
    "        plt.plot(history.history[metric], label=f'Train {metric}')\n",
    "    plt.axvline(x=best_epoch, ls = '--', label = \"Best Epoch\", color = \"#CBC318\" )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Training Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Validation Metrics\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for metric in metrics:\n",
    "        val_metric = f'val_{metric}'\n",
    "        if val_metric in history.history:\n",
    "            plt.plot(history.history[val_metric], label=f'Validation {metric}')\n",
    "    plt.axvline(x=best_epoch, ls = '--', label = \"Best Epoch\", color = \"#CBC318\" )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title('Validation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "\n",
    "\n",
    "def PlotPredictions(start,end,label,prediction):\n",
    "  \"\"\"\n",
    "  plots pairs of images, one for the actual fire mask, and the next for the predicted firemask from our model.\n",
    "\n",
    "  Args:\n",
    "  start (int): the index of the first firemask to use\n",
    "  end (int): the index of the last fire mask to use, non-inclusive\n",
    "  label: the actual fire mask\n",
    "  prediction: the predicted fire mask from the model\n",
    "  \"\"\"\n",
    "    \n",
    "  n_plots = end - start\n",
    "  fig, axes = plt.subplots(n_plots, 2, figsize=(12, 4 * n_plots))\n",
    "  # norm2 = colors.Normalize(vmin=0, vmax=1)  \n",
    "\n",
    "\n",
    "  CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "  BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "  NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "\n",
    "\n",
    "  for i in range(n_plots):\n",
    "    #actual fire mask\n",
    "    axes[i,0].imshow(label[i+start,:,:,0],cmap=CMAP,norm=NORM)\n",
    "    axes[i, 0].set_title(f\"Actual Fire Mask {i + start} \")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    #handles prediction, each prediction has a vector of [p_noFire,p_fire], we will use whatever one is larger\n",
    "    axes[i,1].imshow(prediction[start+i,:,:]>THRESHOLD,cmap=CMAP,norm=NORM)\n",
    "    axes[i, 1].set_title(f\"predicted Fire Mask {i + start}\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def show_preds(inputs, labels, preds, index=0):\n",
    "    CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "    BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "    NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "    axs[0].imshow(inputs[index, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "    axs[0].set_title(\"Previous Fire Mask\")\n",
    "    \n",
    "    axs[1].imshow(labels[index, :, :, 0], cmap=CMAP, norm=NORM)\n",
    "    axs[1].set_title(\"True Fire Mask\")\n",
    "\n",
    "    axs[3].imshow(preds[index, :, :, 0], cmap='inferno')\n",
    "    axs[3].set_title(\"Predicted Fire Prob.\")\n",
    "\n",
    "    axs[2].imshow(preds[index, :, :, 0]>THRESHOLD, cmap=CMAP, norm=NORM)\n",
    "    axs[2].set_title(\"Predicted Fire Mask\")\n",
    "    \n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pMask = model.predict(inputs)\n",
    "\n",
    "\n",
    "#apply neigborhood filtering\n",
    "\n",
    "# filtered_preds = np.array([\n",
    "#     neighborhood_filter(mask, threshold=0.2, required_fraction=0.2)\n",
    "#     for mask in pMask[:,:,:,0]\n",
    "# ])\n",
    "\n",
    "\n",
    "# plotRows(0,5,titles,len(titles)-1, train)\n",
    "# PlotPredictions(0,15,labels,pMask)\n",
    "# PlotPredictions(0,15,labels,filtered_preds)\n",
    "for i in range(15):\n",
    "    show_preds(inputs,labels,pMask,index = i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pMask_eval = model.predict(e_inputs)\n",
    "\n",
    "PlotPredictions(0,15,e_labels,pMask_eval)\n",
    "# PlotPredictions(0,15,labels,filtered_preds)\n",
    "for i in range(15):\n",
    "    show_preds(e_inputs,e_labels,pMask_eval,index = i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
